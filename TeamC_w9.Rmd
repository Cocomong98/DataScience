---
title: "Lab6_TeamC" 
author: "21900251 Noah Ma 21900707 Young Woo Cho 21901015 Kim Yong Hyeon 22000108 Sooa Kim 22100321 Hanvi Park"
date: "05/02/2023"
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: vignette
---

get datas
```{r}
PRSA_data <- load("/Users/cocomong_98/PRSA_data.RData")
str(PRSA_data)
```

# seperate data to  train_data, test_data
```{r}
PRSA_data
```

#install.packages("ROCR")
```{r}
library(ROCR)
library(dplyr)
```

#1 Check data types, check NA in train and test, and find the period of concentration
# Clear NA in pm 2.5
# Judging from the erasure, the NA value is 
#Clear and check


```{r}
train_data
test_data
train_data <- train_data[!is.na(train_data$pm2.5), ]
test_data <- test_data[!is.na(test_data$pm2.5), ]
train_data
test_data
```



#2-1

# Train a single variable model using Month variable
# Set threshold to 0.5 and make predictions
# Calculate confusion matrix for train and test data
# Calculate accuracy, precision, and recall using prop.table function
# Print results

```{r}
model <- glm(pm2.5 ~ Month, data=train_data, family=binomial)
threshold <- 0.5
train_preds <- ifelse(predict(model, train_data, type="response") > threshold, 1, 0)
test_preds <- ifelse(predict(model, test_data, type="response") > threshold, 1, 0)
train_cm <- table(train_preds, train_data$pm2.5)
test_cm <- table(test_preds, test_data$pm2.5)
train_acc <- sum(diag(train_cm))/sum(train_cm)
test_acc <- sum(diag(test_cm))/sum(test_cm)
train_precision <- prop.table(train_cm, margin=1)[2,2] / sum(train_preds)
test_precision <- prop.table(test_cm, margin=1)[2,2] / sum(test_preds)
train_recall <- prop.table(train_cm, margin=2)[2,2] / sum(train_data$pm2.5 == 1)
test_recall <- prop.table(test_cm, margin=2)[2,2] / sum(test_data$pm2.5 == 1)
cat("Accuracy for train data:", round(train_acc, 2), "\n")
cat("Accuracy for test data:", round(test_acc, 2), "\n")
```

#2-2
# Train the model
# Make predictions for train and test data
# Calculate AUC for train and test data
#To output up to the second decimal place, use the round function
#Plot ROC curve for train data
# Plot ROC curve for test data

```{r}
model <- glm(pm2.5 ~ Month, data = train_data, family = binomial(link = "logit"))
train_pred <- predict(model, type = "response", newdata = train_data)
test_pred <- predict(model, type = "response", newdata = test_data)
train_auc <- as.numeric(performance(prediction(train_pred, train_data$pm2.5), "auc")@y.values)
test_auc <- as.numeric(performance(prediction(test_pred, test_data$pm2.5), "auc")@y.values)
train_perf <- performance(prediction(train_pred, train_data$pm2.5), "tpr", "fpr")
dev.new(Width=10, height=8)
par(mar=c(5,5,4,2)+0.1)
plot(train_perf, main = "ROC Curve", colorize = TRUE, print.cutoffs.at = seq(0, 1, 0.1))
cat("AUC value for train data", train_auc, "\n")
test_perf <- performance(prediction(test_pred, test_data$pm2.5), "tpr", "fpr")
dev.new(Width=10, height=8)
par(mar=c(5,5,4,2)+0.1)
plot(test_perf, main = "ROC Curve", colorize = TRUE, print.cutoffs.at = seq(0, 1, 0.1))
cat("AUC value for test data", test_auc, "\n")
```

#2-3
#  The model's accuracy on the test set is lower than its accuracy on the training set. 
#  This is not indicative of overfitting. Because the model is giving different value for 
# a different data set. Also because this is a single variable model looking at only month 
# in comparison to fine dust level the model will not be overfitting.

#2-4
# set train_data$pm2.5 to TRUE or FALSE Value 
# create table with Month and pm2.5 data 
# add this data into sv_model_month which we will use as train_data$est_prob model 
# Using threshold we can now continue to add prediction value to train$prediction
# Now we create new dataframe threshold_s to add the values of thresholds
# precision and recall using for loop. 
# make conf.table to calculate precision and recall in the for loop then add these
# values to the data frame. 
# when one loop is finished add 0.02 to increase the threshold and go through the loop again.
```{r}
train_data$pm2.5  <- ifelse(train_data$pm2.5 ==  "HIGH", TRUE, FALSE)  
table(train_data$pm2.5)
tble <- table(train_data$Month,train_data$pm2.5)
sv_model_month <- prop.table(tble, margin=1)[,2]
train_data$est_prob <- sv_model_month[train_data$Month]
train_data$prediction <- train_data$est_prob > threshold

threshold_s <- data.frame(thresholds = numeric(), precision = numeric(), recall = numeric())
thresholds <- 0.45

for (i in 1:6) {
  
  train_data$prediction <- train_data$est_prob > thresholds 
  conf.table_for_comp <- table(pred = train_data$prediction, actual = train_data$pm2.5)
  precision <- conf.table_for_comp[2,2]/sum(conf.table_for_comp[2,])
  recall <- conf.table_for_comp[2,2]/ sum(conf.table_for_comp[,2])
  threshold_s <- rbind(threshold_s, data.frame(thresholds = thresholds, precision = precision, recall = recall))
  thresholds <- thresholds + 0.02
}
threshold_s
```

#2-5
#according to the dat as we increase the threshold we can see that the precision increases 
# however, the recall decreases. In this case the recall would be the amount of correctly identified dates
# and the precision will be how accurately the model predicts the the finedust level in the air. 
# If I want to know for sure if today has a high level of fine dust then I will increase the threshold level to
# a higher threshold. However, I believe that fine dust level is no so threatening to the people so maybe 
# we do not need to sacrifice all the recall so I would use a lower threshold like 0.49. I chose this because 
# I thought that after 0.49 the recall drops significantly and it has the highest precision ebfore recall dropping 
# significantly 

#2-6

# chose precision and recall from 0.49 threshold.

```{r}
thresholds <- 0.49
train_data$prediction <- train_data$est_prob > thresholds 
  conf.table_for_comp <- table(pred = train_data$prediction, actual = train_data$pm2.5)
  precision <- conf.table_for_comp[2,2]/sum(conf.table_for_comp[2,])
  recall <- conf.table_for_comp[2,2]/ sum(conf.table_for_comp[,2])

F1score <- 2*precision*recall / (precision+recall)
F1score
```

#3-1

# The function summarizes the structure and contents of the data frame and outputs it. This code outputs train_data data frame
# Converting a TEMP variable in a data frame to a factor type
#  TEMP and pm2.5 variables in a data frame in cross-table form
# Calculate the ratio of pm2.5 per TEMP using the cross table and store it in a variable called model_temp_pm
# Add a variable called temp_pm_prob to the train_data data frame, and replace that variable with a ratio of pm2.5 per TEMP at model_temp_pm
# Add a variable called temp_pm_prediction to the train_data data frame, and replace the value TRUE if temp_pm_prob is greater than threshold, otherwise FALSE
# Outputs the values of the TEMP, temp_pm_prob, temp_pm_prediction, pm2.5 variables in the train_data data frame. Outputs only the top 10 rows
# Create a cross-table using predicted and actual values
# Calculate accuracy using cross-tables
# Convert data from a TEMP column to factor data in a test_data data frame
# Create an independence table in the test_data data frame with TEMP and pm2.5 columns as rows and columns, respectively
# Use the t_temp_pm independence test table to calculate the probability of pm2.5 appearing per row (TEMP)
# For each row corresponding to the TEMP column in the test_data data frame, add the probability value obtained in number 3 to the t_temp_pm_prob column
# For each row in the t_temp_pm_prob column, classify it as 1 if it is greater than or equal to 0 if it is less than or equal to the threshold set in number 5 and add it to the t_temp_pm_prediction column
# Outputs the first 10 rows of columns TEMP, t_temp_pm_prob, t_temp_pm_prediction, pm2.5 in the test_data data frame
# Create a confusion matrix with the t_temp_pm_prediction columns and pm2.5 columns as predicted values (pred) and actual values, respectively
# Output of the five categories table
# Calculation of accuracy using a misclassification table


```{r}
glimpse(train_data)
train_data$TEMP <- factor(train_data$TEMP)
temp_pm <- table(train_data$TEMP, train_data$pm2.5)
model_temp_pm <- prop.table(temp_pm, margin = 1)[,2]
train_data$temp_pm_prob <- model_temp_pm[train_data$TEMP]
threshold <- 0.5
train_data$temp_pm_prediction <- train_data$temp_pm_prob > threshold
head(train_data[, c('TEMP', 'temp_pm_prob', 'temp_pm_prediction', 'pm2.5')],10)
temp_conf.table <- table(pred = train_data$temp_pm_prediction, actual = train_data$pm2.5)
temp_conf.table
accuracy <- sum(diag(temp_conf.table))/sum(temp_conf.table)
accuracy
test_data$TEMP <- factor(test_data$TEMP)
t_temp_pm <- table(test_data$TEMP, test_data$pm2.5)
t_model_temp_pm <- prop.table(t_temp_pm, margin = 1)[,2]
test_data$t_temp_pm_prob <- t_model_temp_pm[test_data$TEMP]
threshold <- 0.5
test_data$t_temp_pm_prediction <- test_data$t_temp_pm_prob > threshold
head(test_data[, c('TEMP', 't_temp_pm_prob', 't_temp_pm_prediction', 'pm2.5')],10)
t_temp_conf.table <- table(pred = test_data$t_temp_pm_prediction, actual = test_data$pm2.5)
t_temp_conf.table
t_temp_accuracy <- sum(diag(t_temp_conf.table))/sum(t_temp_conf.table)
t_temp_accuracy
```


#3-2
# Create a prediction object using the temp_pm_prob and pm2.5 variables in train_data, extract 'tpr' (true positive rate) and 'fpr' (false positive rate) from the object, and call a plot function to draw a graph
# Define a function named calAUC. This function uses the prediction function to calculate auc, and returns the value. Within the function, perf@y.va lues is the auc value
# Call calAUC function with temp_pm_prob and pm2.5 variables in train_data
# t_temp_pm_prob in test_data, using pm2.5 variables Create a prediction object, extract 'tpr' and 'fpr' from an object, call a plot function to graph
#  t_temp_pm_prob in test_data, use pm2.5 variable to call calAUC function

```{r}
plot(performance(prediction(train_data$temp_pm_prob, train_data$pm2.5), 'tpr', 'fpr'))
calAUC <- function(predCol, targetCol){
  perf <- performance(prediction(predCol, targetCol), 'auc')
  as.numeric(perf@y.values)
}
calAUC(train_data$temp_pm_prob, train_data$pm2.5)
plot(performance(prediction(test_data$t_temp_pm_prob, test_data$pm2.5), 'tpr', 'fpr'))
calAUC(test_data$t_temp_pm_prob, test_data$pm2.5)
```

#3-3
# Looking at the results, it can be seen that the accuracy of test_data is higher than that of train_data. In general, test_data cannot be more accurate than train_data. So this data can be said to be overfitted.

#3-4

# Create a 6x3 matrix named temp_mat to store the precision and recall. Column name of matrix 'threshold', 'precision', 'recall'
# Set to threshold 0.43
# Repeat 6 times with for loop
# 0.02 added to threshold
# Apply train_data$temp_pm_prob>threshold to generate prediction results
# Compute the confusion matrix for the generated prediction results and actual values and save them to temp_conf.table
# Calculate the precision and recall using temp_conf.table
# Save calculated threshold, precision, and recall values to temp_Row
# Save sequentially to temp_mat


```{r}
temp_conf.table
temp_mat <- matrix(0, nrow = 6, ncol = 3)
colnames(temp_mat) <- c('threshold', 'precision', 'recall')
threshold <- 0.43
for(i in 1:6){
  threshold <- threshold + 0.02
  train_data$temp_pm_prediction <- train_data$temp_pm_prob > threshold
  temp_conf.table <- table(pred = train_data$temp_pm_prediction, actual = train_data$pm2.5)
  temp_precision <- temp_conf.table[2,2] / sum(temp_conf.table[2,])
  temp_recall <- temp_conf.table[2,2] / sum(temp_conf.table[,2])
  temp_Row <- c(threshold, temp_precision, temp_recall)
  temp_mat[i,] <- temp_Row
}
temp_mat
```


#3-5
In #3-4, the average is best when the number is 0.45, which is the highest
```{r}
rowSums(temp_mat[, 2:3])
```


#3-6

#Reset to the most suitable threshold and recalculate precision and recall
#Calculate the given F1score using the precision and recall calculated earlier

```{r}
threshold <- 0.45
train_data$temp_pm_prediction <- train_data$temp_pm_prob > threshold
temp_conf.table <- table(pred = train_data$temp_pm_prediction, actual = train_data$pm2.5)
temp_precision <- temp_conf.table[2,2] / sum(temp_conf.table[2,])
temp_recall <- temp_conf.table[2,2] / sum(temp_conf.table[,2])
temp_precision
temp_recall

temp_F1score <- 2*temp_precision*temp_recall / (temp_precision+temp_recall)
temp_F1score
```

#4-1

# Train a single variable model using Month variable
# Set threshold to 0.5 and make predictions
# Calculate confusion matrix for train and test data
# Calculate accuracy, precision, and recall using prop.table function
# Print results


```{r}
model <- glm(pm2.5 ~ Iws, data=train_data, family=binomial)
threshold <- 0.5
train_preds <- ifelse(predict(model, train_data, type="response") > threshold, 1, 0)
test_preds <- ifelse(predict(model, test_data, type="response") > threshold, 1, 0)
train_cm <- table(train_preds, train_data$pm2.5)
test_cm <- table(test_preds, test_data$pm2.5)
train_acc <- sum(diag(train_cm))/sum(train_cm)
test_acc <- sum(diag(test_cm))/sum(test_cm)
train_precision <- prop.table(train_cm, margin=1)[2,2] / sum(train_preds)
test_precision <- prop.table(test_cm, margin=1)[2,2] / sum(test_preds)
train_recall <- prop.table(train_cm, margin=2)[2,2] / sum(train_data$pm2.5 == 1)
test_recall <- prop.table(test_cm, margin=2)[2,2] / sum(test_data$pm2.5 == 1)
cat("Accuracy for train data:", round(train_acc, 2), "\n")
cat("Accuracy for test data:", round(test_acc, 2), "\n")

```

#4-2

# Train the model
# Make predictions for train and test data
# Calculate AUC for train and test data
#To output up to the second decimal place, use the round function
#Plot ROC curve for train data
# Plot ROC curve for test data

```{r}
model <- glm(pm2.5 ~ Iws, data = train_data, family = binomial(link = "logit"))
train_pred <- predict(model, type = "response", newdata = train_data)
test_pred <- predict(model, type = "response", newdata = test_data)
train_auc <- as.numeric(performance(prediction(train_pred, train_data$pm2.5), "auc")@y.values)
test_auc <- as.numeric(performance(prediction(test_pred, test_data$pm2.5), "auc")@y.values)
train_perf <- performance(prediction(train_pred, train_data$pm2.5), "tpr", "fpr")
dev.new(Width=10, height=8)
par(mar=c(5,5,4,2)+0.1)
plot(train_perf, main = "ROC Curve", colorize = TRUE, print.cutoffs.at = seq(0, 1, 0.1))
cat("AUC value for train data", train_auc, "\n")
test_perf <- performance(prediction(test_pred, test_data$pm2.5), "tpr", "fpr")
dev.new(Width=10, height=8)
par(mar=c(5,5,4,2)+0.1)
plot(test_perf, main = "ROC Curve", colorize = TRUE, print.cutoffs.at = seq(0, 1, 0.1))
cat("AUC value for test data", test_auc, "\n")
```

#4-3
#Based on the results obtained in questions 4-1 and 4-2, it seems that the model is overfitting. 
#The accuracy on the training data is higher than the accuracy on the test data, which is usually an indication of overfitting. 
#Additionally, the AUC value for the training data is higher than the AUC value for the test data, which is another indication of overfitting.

#4-4
#안됨
# Define function to calculate precision and recall for different thresholds
# Create empty data frame to store results
# Loop through each threshold value
# Calculate predicted class labels based on threshold
# Calculate precision and recall
# Store results in data frame
# Return data frame
# Define thresholds to test
# Calculate precision and recall for different thresholds on train data
# Calculate precision and recall for different thresholds on test data
# Print the results

```{r}
calculate_pr <- function(model, data, threshold_seq) {
  results <- data.frame(threshold = numeric(),
                        precision = numeric(),
                        recall = numeric(),
                        stringsAsFactors = T)
  
  for (i in 1:length(threshold_seq)) {
    pred <- ifelse(predict(model, data, type="response") > threshold_seq[i], 1, 0)
    tp <- sum(pred == 1 & data$pm2.5 == 1)
    fp <- sum(pred == 1 & data$pm2.5 == 0)
    fn <- sum(pred == 0 & data$pm2.5 == 1)
    
    precision <- tp / (tp + fp)
    recall <- tp / (tp + fn)
    
    results[i, ] <- c(threshold = threshold_seq[i],
                      precision = precision,
                      recall = recall)
  }
  return(results)
}
threshold_seq <- seq(0, 1, 0.02)
train_pr <- calculate_pr(model, train_data, threshold_seq)
test_pr <- calculate_pr(model, test_data, threshold_seq)
cat("Train data:\n")
print(train_pr[, c("threshold", "precision", "recall")])
cat("Test data:\n")
print(test_pr[, c("threshold", "precision", "recall")])
```

#4-5
When the average is shown in #4-4, it is most suitable when the number is 0.47, which is the highest.

#4-6
# Calculate precision and recall for best threshold
# Calculate F1 score

```{r}
best_threshold <- 0.47
best_preds <- ifelse(predict(model, test_data, type = "response") > best_threshold, 1, 0)
best_cm <- table(best_preds, test_data$pm2.5)
best_precision <- prop.table(best_cm, margin = 1)[2,2] / sum(best_preds)
best_recall <- prop.table(best_cm, margin = 2)[2,2] / sum(test_data$pm2.5 == 1)
f1_score <- 2 * (best_precision * best_recall) / (best_precision + best_recall)
cat("F1 Score: ", f1_score, "\n")
```


# 5-1
#The number of H and L in each month
# Save only the probability of H
#Save to existing data
#True False Prediction & Storage
#The number of H and L in each month
# Save only the probability of H
#Save to existing data
#True False Prediction & Storage

```{r}
tble <- table(train_data$time, train_data$pm2.5) 
sv_model_time <- prop.table(tble, margin = 1)[,2] 
train_data$est_prop <- sv_model_time[train_data$time] 
train_data$prediction <- train_data$est_prop > 0.5 
conf.table <- table(pred=train_data$prediction, actual=train_data$pm2.5)
accuracy <- (conf.table[1,2]+conf.table[2,1]) / sum(conf.table)
tblet <- table(test_data$time, test_data$pm2.5) 
sv_model_time_t <- prop.table(tblet, margin = 1)[,2] 
test_data$est_prop <- sv_model_time_t[test_data$time] 
test_data$prediction <- test_data$est_prop > 0.5 
conf.tablet <- table(pred=test_data$prediction, actual=test_data$pm2.5)
accuracy_t <- (conf.tablet[1,2]+conf.tablet[2,1]) / sum(conf.tablet)
```

#5-2

```{r}
plot(performance(prediction(train_data$est_prop, train_data$pm2.5),
                 'tpr', 'fpr'))
a <- performance(prediction(train_data$est_prop, train_data$pm2.5), 'auc')
as.numeric(a@y.values)

plot(performance(prediction(test_data$est_prop, test_data$pm2.5),
                 'tpr', 'fpr'))
b <- performance(prediction(test_data$est_prop, test_data$pm2.5), 'auc')
as.numeric(b@y.values)
```


#5-3
#'''과적합 문제'''

#5-4

```{r}
threshold <- 0.55
test_data$prediction <- test_data$est_prop > threshold
conf.tablet <- table(pred=test_data$prediction, actual=test_data$pm2.5)
precision <- conf.tablet[1,1]/(conf.tablet[1,1]+conf.tablet[2,1])
precision
recal <- conf.tablet[1,1]/(conf.tablet[1,1]+conf.tablet[1,2])
recal
```

#5-5
#'''Threshold 설정문제'''

#5-6
F1 <- 2*precision*recal/(precision+recal)

#6
t_temp_accuracy